{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjX5q2GG6UlI"
   },
   "source": [
    "\n",
    "**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.\n",
    "Remember to update your Course 1 notebook with links to your own work once completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4nsHz4f6Vjh"
   },
   "source": [
    "# Mini-project 5.3 Detecting the anomalous activity of a ship’s engine\n",
    "\n",
    "**Welcome to your first mini-project: Detecting the anomalous activity of a ship’s engine!**\n",
    "\n",
    "This mini-project allows you to dive deep into a real-world challenge, applying and honing the data science skills you've been cultivating so far. In this immersive exploration into detecting the anomalous activity of a ship’s engine, you can practically apply the concepts you've learned over the past few weeks.\n",
    "\n",
    "A poorly maintained ship engine in the supply chain industry can lead to inefficiencies, increased fuel consumption, higher risks of malfunctions, and potential safety hazards. Your challenge in this project is to apply critical thinking and ML concepts to design and implement a robust anomaly detection model.\n",
    "\n",
    "Please set aside approximately **12 hours** to complete the mini-project.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## **Business context**\n",
    "You are provided with a real data set to identify anomalous activity in a ship’s engine functionality (Devabrat,  2022). As you work through this project, keep in mind that, typically speaking, anomalies would make up a minority of the data points (i.e., about 1% to 5% of the data points would be anomalies).\n",
    "\n",
    "The data set contains six important features continuously monitored to evaluate the engine's status as ‘good’ or ‘bad’. These features are:\n",
    "- **Engine rpm (revolutions per minute):** A high rpm indicates the engine is operating at a higher speed than designed for prolonged periods, which can lead to overheating, excessive wear, and eventual failure. A low rpm could signal a lack of power, issues with fuel delivery, or internal mechanical problems.\n",
    "- **Lubrication oil pressure:** Low lubrication oil pressure indicates insufficient lubrication, leading to increased friction, overheating, and engine damage. A high lubrication oil pressure could signal a blockage in the oil delivery system, potentially causing seal or gasket failure.\n",
    "- **Fuel pressure:** High fuel pressure can cause poor engine performance and incomplete combustion, indicating fuel pump or filter issues. A low fuel pressure may result in excessive fuel consumption, poor emissions, or damage to the fuel injectors.\n",
    "- **Coolant pressure:** Low coolant pressure indicates a potential leak in the cooling system or a coolant pump failure, risking engine overheating. A high coolant pressure could be a sign of a blockage in the cooling system or a failing head gasket, which can also lead to overheating.\n",
    "- **Lubrication oil temperature:** High lubrication oil temperature suggests the oil is overheating, which can degrade its lubricating properties and lead to engine damage. A low lubrication oil temperature may indicate it is not reaching its optimal operating temperature, potentially causing inadequate lubrication.\n",
    "- **Coolant temperature:** High coolant temperature signals overheating, which various issues, including a failed thermostat, coolant leak, or insufficient coolant flow can cause. A low coolant temperature could suggest the engine is not reaching its optimal operating temperature, affecting performance and efficiency.\n",
    "\n",
    "Issues with engines could lead to engine malfunctions, potential safety hazards, and downtime (e.g. delayed deliveries), resulting in the breakdown of a ship’s overall functionality, consequently impacting the business, such as affecting revenue via failure to deliver goods. By predicting timely maintenance, the business aims to increase profit by reducing downtime, reducing safety risks for the crew, limiting fuel consumption, and increasing customer satisfaction through timely deliveries.\n",
    "\n",
    "Your task is to develop a robust anomaly detection system to protect a company’s shipping fleet by evaluating engine functionality. Therefore, you’ll explore the data and:\n",
    "- employ preprocessing and feature engineering\n",
    "- perform anomaly detection.\n",
    "\n",
    "You must prepare a report illustrating your insights to the prospective stakeholders, showing how your solution will save the business money and build trust with its stakeholders. At this stage of the project, the main question you need to consider is:\n",
    "- What insights can be gained from the data, and what recommendations can be made to the company based on these insights? For example, which features need to be monitored closely, and what are the thresholds for anomalous observations? Which statistical or ML technique is the best for anomaly detection based on **this data set**, and which feature (univariate approach) or combination of features (multivariate approach) can predict maintenance?\n",
    "\n",
    "<br></br>\n",
    "\n",
    "> **Disclaimer**\n",
    ">\n",
    "> Please note that although a real-life data set was provided, the business context in this project is fictitious. Any resemblance to companies and persons (living or dead) is coincidental. The course designers and hosts assume no responsibility or liability for any errors or omissions in the content of the business context and data sets. The information in the data sets is provided on an 'as is' basis with no guarantees of completeness, accuracy, usefulness, or timeliness.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## **Objective**\n",
    "By the end of this mini-project, you will be able to understand and apply statistical and ML methods for detecting anomalies.\n",
    "\n",
    "In the Notebook, you will:\n",
    "- explore the data set\n",
    "- preprocess the data and conduct feature engineering\n",
    "- apply statistical techniques to detect anomalies\n",
    "- use ML algorithms to detect anomalies.\n",
    "\n",
    "You will also write a report summarising the results of your findings and recommendations.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## **Assessment criteria**\n",
    "By completing this project, you will be able to provide evidence that you can:\n",
    "- demonstrate enhanced problem-solving skills and proposed strategic solutions by systematically analysing complex organisational challenges\n",
    "- identify meaningful patterns in complex data to evidence advanced critical and statistical thinking skills\n",
    "- select statistical techniques appropriate to a solutions design approach and evidence the ability to evaluate their effectiveness\n",
    "- demonstrate enhanced data representation and improved model performance by systematically implementing relevant techniques\n",
    "- design innovative solutions through critically selecting, evaluating and implementing effective unsupervised learning techniques.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## **Project guidance**\n",
    "1. Import the required libraries and data set with the provided URL.\n",
    "2. View the DataFrame and perform EDA, including identifying missing or duplicate values.\n",
    "3. Generate the descriptive statistics of the data, including:\n",
    " - observing the mean for each feature\n",
    " - identifying the median\n",
    " - idenitfying the range values beyond the 95th percentile for at least two features.\n",
    "4. Visualise the data to determine the distribution and extreme values.\n",
    "5. Perform anomaly detection with a statistical method and identify possible anomalies. Specifically:\n",
    "  - Use the interquartile range (IQR) method to identify outliers for each feature.\n",
    "  - Create a new column (corresponding to each feature) that will indicate (in binary – 0,1) if the value of that feature is an outlier as per IQR calculations.\n",
    "  - Use IQR to identify a sample as an outlier only if two or more of the features fall under an outlier category for a particular sample.\n",
    "  - Record your thoughts and oberservations.\n",
    "6. Perform anomaly detection with ML models:\n",
    "  - Perform feature scaling to prepare the data for ML algorithms.\n",
    "  - Using one-class SVM,\n",
    "    - identify possible anomalies\n",
    "    - visualise the output in 2D after performing PCA and ensure the outliers are in a different colour\n",
    "    - apply different combinations of parameter settings to improve the model's outlier predictions to the expected 1-5%\n",
    "    - record your insights about the use of this method.\n",
    "  - Using Isolation Forest,\n",
    "    - identify possible anomalies\n",
    "    - visualise the output in 2D after performing PCA and ensure the outliers are in a different colour\n",
    "    - apply different combinations of parameter settings to improve the model's outlier predictions to the expected 1-5%\n",
    "    - record your insights about the use of this method.\n",
    "7. Document your approach and major inferences from the data analysis and describe which method (and parameters) provided the best results and why.\n",
    "8. When you’ve completed the activity:\n",
    "  - Download your completed Notebook as an IPYNB (Jupyter Notebook) or PY (Python) file. Save the file as follows: LastName_FirstName_CAM_C101_W5_Mini-project.\n",
    "  - Prepare a detailed report (between 800-1000 words) that includes:\n",
    "    - an overview of your approach\n",
    "    - a description of your analysis\n",
    "    - an explanation of the insights you identified\n",
    "    - a summary of which method gave the best results\n",
    "    - a clear visualisation of your anomaly detection approach\n",
    "    - an evaluation of the effectiveness of 2D visualisations in highlighting outliers\n",
    "  - Save the document as a PDF named according to the following convention: LastName_FirstName_CAM_C101_W5_Mini-project.pdf.\n",
    "  - You can submit your files individually or as a ZIP file. If you upload a ZIP file, use the correct naming convention: LastName_FirstName_CAM_C101_W5_Mini-project.zip.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "> **Declaration**\n",
    ">\n",
    "> By submitting your project, you indicate that the work is your own and has been created with academic integrity. Refer to the Cambridge plagiarism regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bljr0Oql_8Gf"
   },
   "source": [
    "> Start your activity here. Select the pen from the toolbar to add your entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm_LdzJ0DHLV"
   },
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOiu3l0CDQB9"
   },
   "outputs": [],
   "source": [
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_XAUAOGDR9T"
   },
   "outputs": [],
   "source": [
    "# 2. Load the Data\n",
    "url = 'https://raw.githubusercontent.com/fourthrevlxd/cam_dsb/main/engine.csv'\n",
    "data = pd.read_csv(url)\n",
    "print('Data shape:', data.shape)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjE3-neADaTO"
   },
   "outputs": [],
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "# 3.1 Basic Information\n",
    "data.info()\n",
    "\n",
    "# 3.2 Check for missing values\n",
    "print('Missing values per column:')\n",
    "print(data.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.3 Check for duplicate values\n",
    "print('Number of duplicate rows:', data.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.4 Statistical Description\n",
    "print(data.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.5 Median Extraction\n",
    "print('Median values for each feature:')\n",
    "print(data.median())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.6 Plot Histograms\n",
    "for col in data.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 3.7 Plot Boxplots\n",
    "for col in data.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpLzcmxbDi8r"
   },
   "outputs": [],
   "source": [
    "# 4. IQR Statistical Outlier Detection\n",
    "# 4.1 Calculate IQR and Create Binary Columns\n",
    "iqr_outliers = pd.DataFrame()\n",
    "for feature in data.columns:\n",
    "    Q1 = data[feature].quantile(0.25)\n",
    "    Q3 = data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    iqr_outliers[feature+'_outlier'] = ((data[feature] < lower_bound) | (data[feature] > upper_bound)).astype(int)\n",
    "\n",
    "# 4.2 View the new binary outlier columns\n",
    "print('First few rows of IQR Outlier Binary Columns:')\n",
    "print(iqr_outliers.head())\n",
    "\n",
    "# 4.3 Mark samples as anomaly if more than 2 features are outliers\n",
    "data['iqr_anomaly'] = (iqr_outliers.sum(axis=1) >= 2).astype(int)\n",
    "\n",
    "# View IQR Anomalies\n",
    "print('Anomalies detected via IQR:', data['iqr_anomaly'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfClvcO5Dmqr"
   },
   "outputs": [],
   "source": [
    "# 5. One-Class SVM Anomaly Detection\n",
    "# 5.1 Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data[data.columns[:6]])\n",
    "\n",
    "# 5.2 Fit the One-Class SVM\n",
    "model_svm = OneClassSVM(kernel='rbf', gamma=0.5, nu=0.02)\n",
    "model_svm.fit(X_scaled)\n",
    "\n",
    "# 5.3 Predict anomalies\n",
    "y_pred_svm = model_svm.predict(X_scaled)\n",
    "\n",
    "# Create a DataFrame\n",
    "svm_df = pd.DataFrame(X_scaled, columns=data.columns[:6])\n",
    "svm_df['anomaly'] = y_pred_svm\n",
    "\n",
    "# View SVM anomalies\n",
    "anomalies_svm = svm_df[svm_df['anomaly'] == -1]\n",
    "print('Anomalies detected by One-Class SVM:', anomalies_svm.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSDTr1u7Do4j"
   },
   "outputs": [],
   "source": [
    "# 6. Isolation Forest Anomaly Detection\n",
    "# 6.1 Fit Isolation Forest\n",
    "model_iso = IsolationForest(contamination=0.02, random_state=42)\n",
    "model_iso.fit(X_scaled)\n",
    "\n",
    "# 6.2 Predict anomalies\n",
    "y_pred_iso = model_iso.predict(X_scaled)\n",
    "\n",
    "# Create DataFrame\n",
    "iso_df = pd.DataFrame(X_scaled, columns=data.columns[:6])\n",
    "iso_df['anomaly'] = y_pred_iso\n",
    "\n",
    "# View Isolation Forest anomalies\n",
    "anomalies_iso = iso_df[iso_df['anomaly'] == -1]\n",
    "print('Anomalies detected by Isolation Forest:', anomalies_iso.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UDajjMqDr2F"
   },
   "outputs": [],
   "source": [
    "# 7. PCA 2D Visualization\n",
    "# Reduce features to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 7.1 Plot One-Class SVM Anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=np.where(y_pred_svm == 1, 'Normal', 'Anomaly'), palette={'Normal': 'blue', 'Anomaly': 'red'}, style=np.where(y_pred_svm == -1, 'Anomaly', 'Normal'), s=50)\n",
    "plt.title('One-Class SVM Anomaly Detection (PCA Reduced)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# 7.2 Plot Isolation Forest Anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=np.where(y_pred_iso == 1, 'Normal', 'Anomaly'), palette={'Normal': 'blue', 'Anomaly': 'red'}, style=np.where(y_pred_iso == -1, 'Anomaly', 'Normal'), s=50)\n",
    "plt.title('Isolation Forest Anomaly Detection (PCA Reduced)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxBHjRuzDu43"
   },
   "outputs": [],
   "source": [
    "# 8. Summary and Conclusion\n",
    "print('Summary:')\n",
    "print(f'IQR detected {data[\"iqr_anomaly\"].sum()} anomalies.')\n",
    "print(f'One-Class SVM detected {anomalies_svm.shape[0]} anomalies.')\n",
    "print(f'Isolation Forest detected {anomalies_iso.shape[0]} anomalies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjrtAugN6rBz"
   },
   "source": [
    "# Reflect\n",
    "\n",
    "Write a brief paragraph highlighting your process and the rationale to showcase critical thinking and problem-solving.\n",
    "\n",
    "> Select the pen from the toolbar to add your entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggnYxWfMMvuj"
   },
   "source": [
    "### Reference:\n",
    "Devabrat, M., 2022. Predictive Maintenance on Ship's Main Engine using AI. Available at: https://dx.doi.org/10.21227/g3za-v415. [Accessed 5 March 2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini‑Project 5.3 — Ship Engine Anomaly Detection (Resubmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Michail Naoum  \n",
    "**Date:** 2025-10-12  \n",
    "\n",
    "This resubmission notebook adds:\n",
    "- Full EDA summary (mean, median, **95th percentile**)\n",
    "- IQR thresholds + **K‑feature rule sweep (K = 1..4)** and selected K targeting **1–5%** anomalies\n",
    "- **One‑Class SVM** parameter sweep (ν, γ) with anomaly rates\n",
    "- **Isolation Forest (unscaled)** parameter sweep (contamination, n_estimators) + (scaled) comparison\n",
    "- **PCA explained variance ratio** and **2D plots** for each method\n",
    "- Comparison table + **Jaccard overlaps**\n",
    "- Recommendations & notes for the written report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA — Summary Table (Mean, Median, 95th Percentile)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:36:03.520447Z",
     "start_time": "2025-10-18T08:36:01.668139Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Expect a DataFrame named `df` with 6 numeric columns (as per original notebook).\n",
    "# If not present yet, try to load using the original URL (kept for completeness).\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    import pandas as pd\n",
    "    URL = \"https://raw.githubusercontent.com/fourthrevlxd/cam_dsb/main/engine.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "    except Exception as e:\n",
    "        print(\"Data not loaded automatically. Please ensure `df` exists. Error:\", e)\n",
    "\n",
    "assert isinstance(df, pd.DataFrame), \"Please define `df` as your engine dataset DataFrame.\"\n",
    "\n",
    "eda_tbl = df.describe(percentiles=[0.95]).T.rename(columns={\"50%\":\"median\"})\n",
    "eda_tbl = eda_tbl[[\"mean\", \"median\", \"95%\", \"min\", \"max\"]]\n",
    "display(eda_tbl.style.set_caption(\"EDA Summary: mean/median/95th/min/max\"))\n",
    "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "print(\"Duplicate rows:\", int(df.duplicated().sum()))\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10624b710>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f3637\">\n",
       "  <caption>EDA Summary: mean/median/95th/min/max</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f3637_level0_col0\" class=\"col_heading level0 col0\" >mean</th>\n",
       "      <th id=\"T_f3637_level0_col1\" class=\"col_heading level0 col1\" >median</th>\n",
       "      <th id=\"T_f3637_level0_col2\" class=\"col_heading level0 col2\" >95%</th>\n",
       "      <th id=\"T_f3637_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
       "      <th id=\"T_f3637_level0_col4\" class=\"col_heading level0 col4\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row0\" class=\"row_heading level0 row0\" >Engine rpm</th>\n",
       "      <td id=\"T_f3637_row0_col0\" class=\"data row0 col0\" >791.239263</td>\n",
       "      <td id=\"T_f3637_row0_col1\" class=\"data row0 col1\" >746.000000</td>\n",
       "      <td id=\"T_f3637_row0_col2\" class=\"data row0 col2\" >1324.000000</td>\n",
       "      <td id=\"T_f3637_row0_col3\" class=\"data row0 col3\" >61.000000</td>\n",
       "      <td id=\"T_f3637_row0_col4\" class=\"data row0 col4\" >2239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row1\" class=\"row_heading level0 row1\" >Lub oil pressure</th>\n",
       "      <td id=\"T_f3637_row1_col0\" class=\"data row1 col0\" >3.303775</td>\n",
       "      <td id=\"T_f3637_row1_col1\" class=\"data row1 col1\" >3.162035</td>\n",
       "      <td id=\"T_f3637_row1_col2\" class=\"data row1 col2\" >5.058040</td>\n",
       "      <td id=\"T_f3637_row1_col3\" class=\"data row1 col3\" >0.003384</td>\n",
       "      <td id=\"T_f3637_row1_col4\" class=\"data row1 col4\" >7.265566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row2\" class=\"row_heading level0 row2\" >Fuel pressure</th>\n",
       "      <td id=\"T_f3637_row2_col0\" class=\"data row2 col0\" >6.655615</td>\n",
       "      <td id=\"T_f3637_row2_col1\" class=\"data row2 col1\" >6.201720</td>\n",
       "      <td id=\"T_f3637_row2_col2\" class=\"data row2 col2\" >12.208475</td>\n",
       "      <td id=\"T_f3637_row2_col3\" class=\"data row2 col3\" >0.003187</td>\n",
       "      <td id=\"T_f3637_row2_col4\" class=\"data row2 col4\" >21.138326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row3\" class=\"row_heading level0 row3\" >Coolant pressure</th>\n",
       "      <td id=\"T_f3637_row3_col0\" class=\"data row3 col0\" >2.335369</td>\n",
       "      <td id=\"T_f3637_row3_col1\" class=\"data row3 col1\" >2.166883</td>\n",
       "      <td id=\"T_f3637_row3_col2\" class=\"data row3 col2\" >4.438415</td>\n",
       "      <td id=\"T_f3637_row3_col3\" class=\"data row3 col3\" >0.002483</td>\n",
       "      <td id=\"T_f3637_row3_col4\" class=\"data row3 col4\" >7.478505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row4\" class=\"row_heading level0 row4\" >lub oil temp</th>\n",
       "      <td id=\"T_f3637_row4_col0\" class=\"data row4 col0\" >77.643420</td>\n",
       "      <td id=\"T_f3637_row4_col1\" class=\"data row4 col1\" >76.817350</td>\n",
       "      <td id=\"T_f3637_row4_col2\" class=\"data row4 col2\" >84.940778</td>\n",
       "      <td id=\"T_f3637_row4_col3\" class=\"data row4 col3\" >71.321974</td>\n",
       "      <td id=\"T_f3637_row4_col4\" class=\"data row4 col4\" >89.580796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3637_level0_row5\" class=\"row_heading level0 row5\" >Coolant temp</th>\n",
       "      <td id=\"T_f3637_row5_col0\" class=\"data row5 col0\" >78.427433</td>\n",
       "      <td id=\"T_f3637_row5_col1\" class=\"data row5 col1\" >78.346662</td>\n",
       "      <td id=\"T_f3637_row5_col2\" class=\"data row5 col2\" >88.612891</td>\n",
       "      <td id=\"T_f3637_row5_col3\" class=\"data row5 col3\" >61.673325</td>\n",
       "      <td id=\"T_f3637_row5_col4\" class=\"data row5 col4\" >195.527912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " Engine rpm          0\n",
      "Lub oil pressure    0\n",
      "Fuel pressure       0\n",
      "Coolant pressure    0\n",
      "lub oil temp        0\n",
      "Coolant temp        0\n",
      "dtype: int64\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IQR Outliers — Thresholds & K‑Feature Rule Sweep"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:36:36.975247Z",
     "start_time": "2025-10-18T08:36:36.956203Z"
    }
   },
   "source": [
    "from typing import Tuple, Dict\n",
    "def iqr_bounds(s: pd.Series, k: float = 1.5) -> Tuple[float, float]:\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    i = q3 - q1\n",
    "    return q1 - k*i, q3 + k*i\n",
    "\n",
    "# Per‑feature outlier flags\n",
    "iqr_flags = pd.DataFrame(index=df.index)\n",
    "bounds: Dict[str, Tuple[float, float]] = {}\n",
    "for col in df.columns:\n",
    "    lo, hi = iqr_bounds(df[col], 1.5)\n",
    "    bounds[col] = (lo, hi)\n",
    "    iqr_flags[f\"{col}_outlier\"] = ((df[col] < lo) | (df[col] > hi)).astype(int)\n",
    "\n",
    "# Threshold table for report\n",
    "thr_table = pd.DataFrame(bounds, index=[\"lower_bound\", \"upper_bound\"]).T\n",
    "display(thr_table.style.set_caption(\"IQR Bounds per Feature\"))\n",
    "\n",
    "# Sweep K = 1..4 and pick K in 1–5% (closest to 3%)\n",
    "flag_sum = iqr_flags.sum(axis=1)\n",
    "rows = []\n",
    "for K in (1,2,3,4):\n",
    "    mask = (flag_sum >= K).astype(int)\n",
    "    rows.append({\"K_features\": K, \"anomaly_count\": int(mask.sum()), \"anomaly_rate\": mask.mean()})\n",
    "iqr_k_table = pd.DataFrame(rows)\n",
    "display(iqr_k_table.style.set_caption(\"IQR: K‑Feature Rule Sweep\"))\n",
    "\n",
    "target = 0.03\n",
    "cands = iqr_k_table[(iqr_k_table[\"anomaly_rate\"] >= 0.01) & (iqr_k_table[\"anomaly_rate\"] <= 0.05)]\n",
    "if not cands.empty:\n",
    "    best_k = int(cands.iloc[(cands[\"anomaly_rate\"]-target).abs().argmin()][\"K_features\"])\n",
    "else:\n",
    "    best_k = int(iqr_k_table.iloc[(iqr_k_table[\"anomaly_rate\"]-target).abs().argmin()][\"K_features\"])\n",
    "\n",
    "df[\"iqr_anomaly\"] = (flag_sum >= best_k).astype(int)\n",
    "print(f\"Chosen K = {best_k} | anomalies = {int(df['iqr_anomaly'].sum())} | rate = {df['iqr_anomaly'].mean():.2%}\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1055f1670>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ccafd\">\n",
       "  <caption>IQR Bounds per Feature</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ccafd_level0_col0\" class=\"col_heading level0 col0\" >lower_bound</th>\n",
       "      <th id=\"T_ccafd_level0_col1\" class=\"col_heading level0 col1\" >upper_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row0\" class=\"row_heading level0 row0\" >Engine rpm</th>\n",
       "      <td id=\"T_ccafd_row0_col0\" class=\"data row0 col0\" >81.500000</td>\n",
       "      <td id=\"T_ccafd_row0_col1\" class=\"data row0 col1\" >1445.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row1\" class=\"row_heading level0 row1\" >Lub oil pressure</th>\n",
       "      <td id=\"T_ccafd_row1_col0\" class=\"data row1 col0\" >0.214130</td>\n",
       "      <td id=\"T_ccafd_row1_col1\" class=\"data row1 col1\" >6.359957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row2\" class=\"row_heading level0 row2\" >Fuel pressure</th>\n",
       "      <td id=\"T_ccafd_row2_col0\" class=\"data row2 col0\" >0.674755</td>\n",
       "      <td id=\"T_ccafd_row2_col1\" class=\"data row2 col1\" >11.987104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row3\" class=\"row_heading level0 row3\" >Coolant pressure</th>\n",
       "      <td id=\"T_ccafd_row3_col0\" class=\"data row3 col0\" >-0.272095</td>\n",
       "      <td id=\"T_ccafd_row3_col1\" class=\"data row3 col1\" >4.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row4\" class=\"row_heading level0 row4\" >lub oil temp</th>\n",
       "      <td id=\"T_ccafd_row4_col0\" class=\"data row4 col0\" >72.207440</td>\n",
       "      <td id=\"T_ccafd_row4_col1\" class=\"data row4 col1\" >81.590241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ccafd_level0_row5\" class=\"row_heading level0 row5\" >Coolant temp</th>\n",
       "      <td id=\"T_ccafd_row5_col0\" class=\"data row5 col0\" >60.365435</td>\n",
       "      <td id=\"T_ccafd_row5_col1\" class=\"data row5 col1\" >96.445397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1132c2d80>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d042e\">\n",
       "  <caption>IQR: K‑Feature Rule Sweep</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d042e_level0_col0\" class=\"col_heading level0 col0\" >K_features</th>\n",
       "      <th id=\"T_d042e_level0_col1\" class=\"col_heading level0 col1\" >anomaly_count</th>\n",
       "      <th id=\"T_d042e_level0_col2\" class=\"col_heading level0 col2\" >anomaly_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d042e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d042e_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_d042e_row0_col1\" class=\"data row0 col1\" >4636</td>\n",
       "      <td id=\"T_d042e_row0_col2\" class=\"data row0 col2\" >0.237318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d042e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d042e_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_d042e_row1_col1\" class=\"data row1 col1\" >422</td>\n",
       "      <td id=\"T_d042e_row1_col2\" class=\"data row1 col2\" >0.021602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d042e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d042e_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_d042e_row2_col1\" class=\"data row2 col1\" >11</td>\n",
       "      <td id=\"T_d042e_row2_col2\" class=\"data row2 col2\" >0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d042e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d042e_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_d042e_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_d042e_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen K = 2 | anomalies = 422 | rate = 2.16%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scaling & PCA — Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:36:47.472723Z",
     "start_time": "2025-10-18T08:36:47.443106Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = df.columns.tolist()\n",
    "X_raw = df[features].to_numpy()\n",
    "\n",
    "# Robust scaling (less sensitive to outliers) and Standard scaling for OCSVM\n",
    "robust = RobustScaler().fit_transform(X_raw)\n",
    "standard = StandardScaler().fit_transform(X_raw)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(robust)\n",
    "print(\"PCA explained variance ratio (2D):\", pca.explained_variance_ratio_, \n",
    "      \" | cumulative:\", pca.explained_variance_ratio_.sum())\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StandardScaler, RobustScaler\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[1;32m      4\u001B[0m features \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist()\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. One‑Class SVM — Parameter Sweep (ν, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "def sweep_ocsvm(X, nus=(0.01,0.02,0.03,0.05), gammas=(\"scale\",\"auto\",0.1,0.5,1.0)):\n",
    "    rows, preds = [], {}\n",
    "    for nu in nus:\n",
    "        for g in gammas:\n",
    "            m = OneClassSVM(kernel=\"rbf\", nu=nu, gamma=g)\n",
    "            m.fit(X)\n",
    "            y = m.predict(X)  # -1 anomaly, 1 normal\n",
    "            rate = (y == -1).mean()\n",
    "            name = f\"OCSVM(nu={nu},gamma={g})\"\n",
    "            rows.append({\"model\": name, \"anomaly_count\": int((y==-1).sum()), \"anomaly_rate\": rate})\n",
    "            preds[name] = y\n",
    "    import pandas as pd\n",
    "    tbl = pd.DataFrame(rows).sort_values(\"anomaly_rate\").reset_index(drop=True)\n",
    "    return tbl, preds\n",
    "\n",
    "oc_tbl, oc_preds = sweep_ocsvm(standard)  # OCSVM with scaled data\n",
    "display(oc_tbl.head(15).style.set_caption(\"OCSVM sweep (top 15 by rate)\"))\n",
    "\n",
    "# pick closest to 3%\n",
    "idx = (oc_tbl[\"anomaly_rate\"] - 0.03).abs().argmin()\n",
    "best_oc = oc_tbl.iloc[idx]\n",
    "best_oc_name = best_oc[\"model\"]\n",
    "y_oc = oc_preds[best_oc_name]\n",
    "print(f\"Chosen {best_oc_name} → {int((y_oc==-1).sum())} anomalies ({(y_oc==-1).mean():.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Isolation Forest — Parameter Sweep (Unscaled Primary, Scaled Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def sweep_if(X, contaminations=(0.01,0.02,0.03,0.05), n_estimators=(200,400)):\n",
    "    rows, preds = [], {}\n",
    "    for c in contaminations:\n",
    "        for n in n_estimators:\n",
    "            m = IsolationForest(contamination=c, n_estimators=n, random_state=42, n_jobs=-1)\n",
    "            m.fit(X)\n",
    "            y = m.predict(X)  # -1 anomaly, 1 normal\n",
    "            rate = (y == -1).mean()\n",
    "            name = f\"IF(c={c},n={n})\"\n",
    "            rows.append({\"model\": name, \"anomaly_count\": int((y==-1).sum()), \"anomaly_rate\": rate})\n",
    "            preds[name] = y\n",
    "    import pandas as pd\n",
    "    tbl = pd.DataFrame(rows).sort_values(\"anomaly_rate\").reset_index(drop=True)\n",
    "    return tbl, preds\n",
    "\n",
    "if_unscaled_tbl, if_unscaled_preds = sweep_if(X_raw)     # PRIMARY per rubric\n",
    "if_scaled_tbl,   if_scaled_preds   = sweep_if(standard)  # comparison\n",
    "\n",
    "display(if_unscaled_tbl.head(12).style.set_caption(\"Isolation Forest (unscaled) — sweep\"))\n",
    "display(if_scaled_tbl.head(12).style.set_caption(\"Isolation Forest (scaled) — sweep\"))\n",
    "\n",
    "# Choose unscaled closest to 3%\n",
    "target = 0.03\n",
    "idx2 = (if_unscaled_tbl[\"anomaly_rate\"] - target).abs().argmin()\n",
    "best_if = if_unscaled_tbl.iloc[idx2]\n",
    "best_if_name = best_if[\"model\"]\n",
    "y_if = if_unscaled_preds[best_if_name]\n",
    "print(f\"Chosen (unscaled) {best_if_name} → {int((y_if==-1).sum())} anomalies ({(y_if==-1).mean():.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PCA 2D Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_pca(X2d, mask, title):\n",
    "    lbl = np.where(mask, \"Anomaly\", \"Normal\")\n",
    "    palette = {\"Normal\":\"tab:blue\",\"Anomaly\":\"tab:red\"}\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.scatterplot(x=X2d[:,0], y=X2d[:,1], hue=lbl, ax=ax, palette=palette, s=22, alpha=0.85)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "mask_iqr = df[\"iqr_anomaly\"].astype(bool).values\n",
    "mask_oc  = (y_oc == -1)\n",
    "mask_if  = (y_if == -1)\n",
    "\n",
    "plot_pca(X_pca, mask_iqr, \"PCA — IQR anomalies\")\n",
    "plot_pca(X_pca, mask_oc,  f\"PCA — {best_oc_name}\")\n",
    "plot_pca(X_pca, mask_if,  f\"PCA — {best_if_name} (unscaled)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparison & Overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "comp = pd.DataFrame({\n",
    "    \"method\": [f\"IQR (K={int(df['iqr_anomaly'].mean()>0) and 'auto'})\", best_oc_name, f\"{best_if_name} (unscaled)\"],\n",
    "    \"anomaly_count\": [int(df[\"iqr_anomaly\"].sum()), int((y_oc==-1).sum()), int((y_if==-1).sum())],\n",
    "    \"anomaly_rate\": [df[\"iqr_anomaly\"].mean(), (y_oc==-1).mean(), (y_if==-1).mean()]\n",
    "})\n",
    "display(comp.style.set_caption(\"Final comparison — counts & rates\"))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    inter = (a & b).sum(); union = (a | b).sum()\n",
    "    return inter/union if union else 0.0\n",
    "\n",
    "overlap = pd.DataFrame({\n",
    "    \"pair\": [\"IQR vs OCSVM\", \"IQR vs IF\", \"OCSVM vs IF\"],\n",
    "    \"jaccard\": [jaccard(mask_iqr,mask_oc), jaccard(mask_iqr,mask_if), jaccard(mask_oc,mask_if)],\n",
    "    \"intersections\": [int((mask_iqr & mask_oc).sum()), int((mask_iqr & mask_if).sum()), int((mask_oc & mask_if).sum())]\n",
    "})\n",
    "display(overlap.style.set_caption(\"Model agreement — Jaccard & intersections\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Recommendations & Notes for Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations**\n",
    "- Use **Isolation Forest** (unscaled) with contamination ≈ **3%** as **primary**; retrain weekly.\n",
    "- Keep **OCSVM** as a **shadow** model; investigate cases where both agree first.\n",
    "- **Alert rule**: trigger when *model = anomaly* **AND** at least **2** IQR feature flags are present.\n",
    "- Track drift: if anomaly rate leaves **1–5%** band, recalibrate parameters/data window.\n",
    "\n",
    "**Reporting notes**\n",
    "- Include **PCA explained variance**.\n",
    "- Include parameter sweep tables (OCSVM, IF) and the **IQR K‑sweep**.\n",
    "- Include the **IQR bounds table** so engineers can map alerts to physical thresholds.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1VCcq2_mgQ6aK_QhjxMBwo5oRMfnUkv6c",
     "timestamp": 1745868365428
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
